groups:
  - name: "prometheus exporter alerts"
    rules:
      - alert: "prometheus exporter has new errors (config reload) P4-5"
        expr: prometheus_config_last_reload_successful{host_priority=~"P[4-5]|^$"} != 1
        for: 10m
        annotations:
          summary: "prometheus config reload fails"
          description: 'Go to host: "{{ $labels.hostname }}" and check why instance: "{{ $labels.instance }}" cannot reload config'
          grafana: "https://grafana.monitoring-test.shalb.com/d/8UZEeGAZz/prometheus-metrics?panelId=5&fullscreen&orgId=1&var-server_env={{ $labels.server_env }}&var-project={{ $labels.project }}&var-hostname={{ $labels.hostname }}&var-instance={{ $labels.instance }}"
          access: "https://confluence.shalb.com/display/TEST/Access#Access-Grafana,Prometheus,Alertmanager"
          priority: "P5"
        labels:
          team: monitoring

      - alert: "prometheus exporter has new errors (config reload) P1-3"
        expr: prometheus_config_last_reload_successful{host_priority=~"P[1-3]"} != 1
        for: 10m
        annotations:
          summary: "prometheus config reload fails"
          description: 'Go to host: "{{ $labels.hostname }}" and check why instance: "{{ $labels.instance }}" cannot reload config'
          grafana: "https://grafana.monitoring-test.shalb.com/d/8UZEeGAZz/prometheus-metrics?panelId=5&fullscreen&orgId=1&var-server_env={{ $labels.server_env }}&var-project={{ $labels.project }}&var-hostname={{ $labels.hostname }}&var-instance={{ $labels.instance }}"
          access: "https://confluence.shalb.com/display/TEST/Access#Access-Grafana,Prometheus,Alertmanager"
          priority: "P3"
        labels:
          team: monitoring


  - name: "prometheus down alerts"
    rules:
      - alert: "prometheus down P4-5"
        expr: up{host_priority=~"P[4-5]|^$",instance=~".*:9090"} == 0
        for: 15m
        annotations:
          summary: "Exporter: {{ $labels.instance }} has new errors"
          description: 'Go to monitoring host: "{{ $labels.hostname }}"  and check log for exporter: "{{ $labels.instance }}"'
          grafana: "https://grafana.monitoring-test.shalb.com/d/8UZEeGAZz/prometheus-metrics?panelId=6&fullscreen&orgId=1&var-server_env={{ $labels.server_env }}&var-project={{ $labels.project }}&var-hostname={{ $labels.hostname }}&var-instance={{ $labels.instance }}"
          access: "https://confluence.shalb.com/display/TEST/Access#Access-Grafana,Prometheus,Alertmanager"
          priority: "P5"
        labels:
          team: monitoring

      - alert: "prometheus down P1-3"
        expr: up{host_priority=~"P[1-3]",instance=~".*:9090"} == 0
        for: 5m
        annotations:
          summary: "Exporter: {{ $labels.instance }} has new errors"
          description: 'Go to monitoring host: "{{ $labels.hostname }}"  and check log for exporter: "{{ $labels.instance }}"'
          grafana: "https://grafana.monitoring-test.shalb.com/d/8UZEeGAZz/prometheus-metrics?panelId=6&fullscreen&orgId=1&var-server_env={{ $labels.server_env }}&var-project={{ $labels.project }}&var-hostname={{ $labels.hostname }}&var-instance={{ $labels.instance }}"
          access: "https://confluence.shalb.com/display/TEST/Access#Access-Grafana,Prometheus,Alertmanager"
          priority: "P2"
        labels:
          team: monitoring


  - name: "prometheus federate alerts"
    rules:
      - alert: "prometheus federate down P4-5"
        expr: up{host_priority=~"P[4-5]|^$",instance=~".*:443",job=~"prometheus_federate.*"} == 0
        for: 15m
        annotations:
          summary: "Exporter: {{ $labels.instance }} has new errors"
          description: 'Go to monitoring host: "{{ $labels.hostname }}" and check log for exporter: "{{ $labels.instance }}"'
          grafana: "https://grafana.monitoring-test.shalb.com/d/8UZEeGAZz/prometheus-metrics?panelId=6&fullscreen&orgId=1&var-server_env={{ $labels.server_env }}&var-project={{ $labels.project }}&var-hostname={{ $labels.hostname }}&var-instance={{ $labels.instance }}"
          access: "https://confluence.shalb.com/display/TEST/Access#Access-Grafana,Prometheus,Alertmanager"
          priority: "P5"
        labels:
          team: monitoring

      - alert: "prometheus federate down P1-3"
        expr: up{host_priority=~"P[1-3]",instance=~".*:443",job=~"prometheus_federate.*"} == 0
        for: 5m
        annotations:
          summary: "Exporter: {{ $labels.instance }} has new errors"
          description: 'Go to monitoring host: "{{ $labels.hostname }}" and check log for exporter: "{{ $labels.instance }}"'
          grafana: "https://grafana.monitoring-test.shalb.com/d/8UZEeGAZz/prometheus-metrics?panelId=6&fullscreen&orgId=1&var-server_env={{ $labels.server_env }}&var-project={{ $labels.project }}&var-hostname={{ $labels.hostname }}&var-instance={{ $labels.instance }}"
          access: "https://confluence.shalb.com/display/TEST/Access#Access-Grafana,Prometheus,Alertmanager"
          priority: "P2"
        labels:
          team: monitoring


  - name: "prometheus connections alerts"
    rules:
      - alert: "prometheus dialer has to many connection errors P4-5"
        expr: sum(delta(net_conntrack_dialer_conn_failed_total{host_priority=~"P[4-5]|^$"}[5m])) without (reason) > 0
        for: 3h
        annotations:
          summary: "prometheus dialer has to many connection errors"
          description: 'Go to monitoring host: "{{ $labels.hostname }}" and check why to many errors for dialer: "{{ $labels.dialer_name }}"'
          grafana: "https://grafana.monitoring-test.shalb.com/d/8UZEeGAZz/prometheus-metrics?panelId=4&fullscreen&orgId=1&var-server_env={{ $labels.server_env }}&var-project={{ $labels.project }}&var-hostname={{ $labels.hostname }}&var-instance={{ $labels.instance }}&var-dialer_name={{ $labels.dialer_name }}"
          access: "https://confluence.shalb.com/display/TEST/Access#Access-Grafana,Prometheus,Alertmanager"
          priority: "P5"
        labels:
          team: monitoring

      - alert: "prometheus dialer has to many connection errors P1-3"
        expr: sum(delta(net_conntrack_dialer_conn_failed_total{host_priority=~"P[1-3]"}[5m])) without (reason) > 0
        for: 1h
        annotations:
          summary: "prometheus dialer has to many connection errors"
          description: 'Go to monitoring host: "{{ $labels.hostname }}" and check why to many errors for dialer: "{{ $labels.dialer_name }}"'
          grafana: "https://grafana.monitoring-test.shalb.com/d/8UZEeGAZz/prometheus-metrics?panelId=4&fullscreen&orgId=1&var-server_env={{ $labels.server_env }}&var-project={{ $labels.project }}&var-hostname={{ $labels.hostname }}&var-instance={{ $labels.instance }}&var-dialer_name={{ $labels.dialer_name }}"
          access: "https://confluence.shalb.com/display/TEST/Access#Access-Grafana,Prometheus,Alertmanager"
          priority: "P3"
        labels:
          team: monitoring


  - name: "prometheus dropped notifications alerts"
    rules:
      - alert: "prometheus: to many dropped notifications P4-5"
        expr: delta(prometheus_notifications_dropped_total{host_priority=~"P[4-5]|^$"}[5m]) > 0
        for: 30m
        annotations:
          summary: "prometheus: to many dropped notifications"
          description: 'Go to monitoring host: "{{ $labels.hostname }}" and check why prometheus insance: "{{ $labels.instance }}" has to many dropped notifications'
          grafana: "https://grafana.monitoring-test.shalb.com/d/8UZEeGAZz/prometheus-metrics?panelId=11&fullscreen&orgId=1&var-server_env={{ $labels.server_env }}&var-project={{ $labels.project }}&var-hostname={{ $labels.hostname }}&var-instance={{ $labels.instance }}"
          access: "https://confluence.shalb.com/display/TEST/Access#Access-Grafana,Prometheus,Alertmanager"
          priority: "P5"
        labels:
          team: monitoring

      - alert: "prometheus: to many dropped notifications P1-3"
        expr: delta(prometheus_notifications_dropped_total{host_priority=~"P[1-3]"}[5m]) > 0
        for: 5m
        annotations:
          summary: "prometheus: to many dropped notifications"
          description: 'Go to monitoring host: "{{ $labels.hostname }}" and check why prometheus insance: "{{ $labels.instance }}" has to many dropped notifications'
          grafana: "https://grafana.monitoring-test.shalb.com/d/8UZEeGAZz/prometheus-metrics?panelId=11&fullscreen&orgId=1&var-server_env={{ $labels.server_env }}&var-project={{ $labels.project }}&var-hostname={{ $labels.hostname }}&var-instance={{ $labels.instance }}"
          access: "https://confluence.shalb.com/display/TEST/Access#Access-Grafana,Prometheus,Alertmanager"
          priority: "P3"
        labels:
          team: monitoring

